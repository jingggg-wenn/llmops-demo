apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: llmops-dev

namePrefix: dev-
nameSuffix: ""

resources:
  - ../../base

patches:
  - target:
      kind: InferenceService
      name: qwen25-05b-instruct
    patch: |-
      - op: replace
        path: /metadata/annotations/openshift.io~1display-name
        value: "qwen2.5-0.5b-dev-v1"
      - op: add
        path: /metadata/annotations/serving.knative.dev~1revision-history-limit
        value: "3"
      - op: add
        path: /spec/predictor/canaryTrafficPercent
        value: 100
      - op: replace
        path: /spec/predictor/model/runtime
        value: "dev-qwen25-05b-instruct"
      - op: replace
        path: /spec/predictor/minReplicas
        value: 1
      - op: replace
        path: /spec/predictor/maxReplicas
        value: 1
      - op: replace
        path: /spec/predictor/model/resources/limits/cpu
        value: "2"
      - op: replace
        path: /spec/predictor/model/resources/limits/memory
        value: "6Gi"
      - op: replace
        path: /spec/predictor/model/resources/requests/cpu
        value: "500m"
      - op: replace
        path: /spec/predictor/model/resources/requests/memory
        value: "4Gi"
# Patch ServingRuntime to change change max model len, GPU memory utilization, enable tool call
  - target:
      kind: ServingRuntime
      name: qwen25-05b-instruct
    patch: |-
      - op: replace
        path: /spec/containers/0/args/4
        value: "12000"
      - op: add
        path: /spec/containers/0/args/-
        value: "--gpu-memory-utilization"
      - op: add
        path: /spec/containers/0/args/-
        value: "0.75"
      - op: add
        path: /spec/containers/0/args/-
        value: "--enable-auto-tool-choice"
      - op: add
        path: /spec/containers/0/args/-
        value: "--tool-call-parser"
      - op: add
        path: /spec/containers/0/args/-
        value: "hermes"
